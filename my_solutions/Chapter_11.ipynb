{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "Inicjalizacja wszystkich wag tą samą wartością to słaby pomysł, ponieważ wszystkie wagi będą miały ten sam wpływ na dane wejściowe. Z tego powodu algorytm gradientu prostego będzie modyfikował wszystkie wagi w ten sam sposób."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "Bias może być inicjalizowany wartością 0. Jest to de facto założenie wyśrodowania danych wejściowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "Zalety SELU:\n",
    "- samonormalizacja\n",
    "- niezerowy gradient dla x < 0\n",
    "- różniczkowalność w każdym punkcie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "- SELU - budujemy sieć samonormalizującą\n",
    "- Leaky ReLU - liczy się szybkość obliczeń\n",
    "- ReLu - jeszcze szybciej niż Leaky ReLU, ale sprawdza się gorzej\n",
    "- tanh - jezeli chcemy aby dane wyjsciowe zawieraly sie w przedziale [-1, 1]\n",
    "- logistyczna - podobnie jak tanh, ale przedzial [0, 1], częściej stosowana w warstwie wyjściowej dla klasyfikacji binarnej\n",
    "- softmax - na warstwie wyjściowej w przypadku klasyfikacji gdzie mamy więcej niż dwie klasy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5\n",
    "Jeżeli moment jest duży. Model może ciągle się rozpędzać w przestrzeni wag, co spowoduje rozbieżność algorytmu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6\n",
    "Sposoby uzyskania modelu rzadkiego:\n",
    "1. Zastosowanie normalizacji l1\n",
    "2. Usuwanie połączeń o wadze poniżej założonego progu\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7\n",
    "Dropout spowalnia proces uczenia. Model potrzebuje więcej epok na dostosowanie się do danych, ale w zamian uzyskujemy mniejszy błąd wariancji. \n",
    "\n",
    "Warstwy dropout wymagają przeskalowania danych przechodzących przez warstwę przez (1-p), gdzie p to prawdopodobieństwo wyłączenia węzła. W trakcie obliczania predykcji wszystkie węzły w sieci są aktywne.\n",
    "\n",
    "W przypadku porzucania MonteCarlo zmiana występuje w przypadku obliczania predykcji. Obliczenie predykcji powtarzamy kilkukrotnie z aktywnymi warstwami Dropout. W rezultacie możemy ocenić wartość oczekiwaną predykcji oraz odchylenie standardowe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
